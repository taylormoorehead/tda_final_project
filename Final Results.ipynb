{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies & Imports**"
      ],
      "metadata": {
        "id": "QqOEiSMFUBVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas_market_calendars ripser persim"
      ],
      "metadata": {
        "id": "8ZRp_81fUBme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import MDS\n",
        "import statsmodels.api as sm\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import networkx as nx\n",
        "from matplotlib.colors import Normalize\n",
        "from scipy.spatial.distance import squareform\n",
        "from tqdm import tqdm\n",
        "import yfinance as yf\n",
        "import pandas_datareader.data as web\n",
        "import pandas_market_calendars as mcal\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.cm as cm\n",
        "from ripser import ripser\n",
        "import networkx as nx\n",
        "from numpy.polynomial.polynomial import Polynomial\n",
        "from persim import plot_diagrams\n",
        "from persim import PersLandscapeExact\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "from sklearn.decomposition import PCA\n",
        "from persim import PersistenceImager\n",
        "from persim.landscapes import PersLandscapeExact\n",
        "from ripser import ripser\n",
        "from persim import plot_diagrams\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "stXgx1-bM_26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Processing**"
      ],
      "metadata": {
        "id": "zLRUfg6DNIex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prices_raw = pd.read_csv('/content/prices.csv')\n",
        "\n",
        "prices_raw['date'] = pd.to_datetime(prices_raw['date'])\n",
        "\n",
        "prices_df = prices_raw.pivot_table(index='date', columns='TICKER', values='PRC', aggfunc='first')\n",
        "\n",
        "prices_df = prices_df.sort_index(axis=1)\n",
        "\n",
        "prices_df = prices_df.abs()\n",
        "\n",
        "mask = pd.DataFrame(False, index=prices_df.index, columns=prices_df.columns)\n",
        "\n",
        "for date, row in components_df.iterrows():\n",
        "    tickers = row['tickers']\n",
        "\n",
        "    valid = list(set(tickers) & set(prices_df.columns))\n",
        "    mask.loc[date, valid] = True\n",
        "\n",
        "prices_df = prices_df.where(mask)\n",
        "\n",
        "spx = yf.download('^GSPC', start='1996-01-01', end='2002-12-31')['Close']\n",
        "prices_df.index = pd.to_datetime(prices_df.index)\n",
        "spx.index = pd.to_datetime(spx.index)\n",
        "prices_df = prices_df.join(spx, how='left')\n",
        "\n",
        "prices_df"
      ],
      "metadata": {
        "id": "66_ZxPCdimHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spx_log_returns = np.log(spx / spx.shift(1)).dropna()\n",
        "component_log_returns = np.log(prices_df / prices_df.shift(1))\n",
        "\n",
        "component_log_returns = component_log_returns.loc[spx_log_returns.index]\n",
        "component_log_returns = component_log_returns.fillna(0)\n",
        "\n",
        "X = component_log_returns\n",
        "y = spx_log_returns\n",
        "\n",
        "model_no_alpha = sm.OLS(y, X).fit()\n",
        "print(model_no_alpha.summary())\n",
        "\n",
        "predicted = model_no_alpha.predict(X).to_numpy().flatten()\n",
        "y_flat = y.to_numpy().flatten()\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Actual': y_flat,\n",
        "    'Synthetic': predicted\n",
        "})\n",
        "\n",
        "ax = comparison.cumsum().plot(title='Cumulative S&P 500 Log Return: Actual vs. Synthetic')\n",
        "ax.set_xlabel(\"Day\")\n",
        "ax.set_ylabel(\"Cumulative Log Return\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bFhC7USFXUX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model_no_alpha.params\n",
        "print(weights)"
      ],
      "metadata": {
        "id": "fm5-vt6RDdQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_weights = weights / weights.sum()\n",
        "print(normalized_weights)"
      ],
      "metadata": {
        "id": "4762uGWxFj6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation & Distance Matrices**"
      ],
      "metadata": {
        "id": "gQq-gTyaHDAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_correlation_array(returns, synthetic, window):\n",
        "    n_stocks = returns.shape[1]\n",
        "    n_windows = len(returns) - window + 1\n",
        "\n",
        "    corr_array = np.zeros((n_windows, n_stocks + 1, n_stocks + 1))\n",
        "\n",
        "    all_returns = np.column_stack([returns.values, synthetic.values])\n",
        "\n",
        "    for i in range(n_windows):\n",
        "        window_data = all_returns[i:i+window, :]\n",
        "        corr_matrix = np.corrcoef(window_data, rowvar=False)\n",
        "        corr_array[i, :, :] = corr_matrix\n",
        "\n",
        "    dates = returns.index[window-1:]\n",
        "\n",
        "    return corr_array, dates\n",
        "\n",
        "window = 126\n",
        "corr_array, dates = rolling_correlation_array(component_returns, synthetic_returns, window)\n",
        "\n",
        "print(corr_array[0, :, :])\n",
        "\n",
        "print(corr_array[0, 0, -1])"
      ],
      "metadata": {
        "id": "u7PfrL4lId3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_to_distance(corr_matrix):\n",
        "\n",
        "    corr_matrix = np.nan_to_num(corr_matrix, nan=0)\n",
        "\n",
        "    distance_matrix = np.sqrt(2 * (1 - corr_matrix))\n",
        "\n",
        "    np.fill_diagonal(distance_matrix, 0)\n",
        "\n",
        "    return distance_matrix"
      ],
      "metadata": {
        "id": "uT7ORS3wHE0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance_array = np.array([correlation_to_distance(m) for m in corr_array])"
      ],
      "metadata": {
        "id": "Z0XUWL6VJl90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Traditional Data Exploration**"
      ],
      "metadata": {
        "id": "lmbVnswIKdtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_corr = corr_array[:, :-1, :-1].mean(axis=(1,2))\n",
        "print(mean_corr)"
      ],
      "metadata": {
        "id": "Cd2znPXvKfXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_vs_index_corr = corr_array[:, :-1, -1]\n",
        "print(stock_vs_index_corr)"
      ],
      "metadata": {
        "id": "Rr90mSHSJm6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_3d_array(arr, name):\n",
        "    n_windows = arr.shape[0]\n",
        "    flat_stats = {\n",
        "        \"mean_corr\": np.mean(arr, axis=(1,2)),\n",
        "        \"std_corr\": np.std(arr, axis=(1,2)),\n",
        "        \"max_corr\": np.max(arr, axis=(1,2)),\n",
        "        \"min_corr\": np.min(arr, axis=(1,2))\n",
        "    }\n",
        "    return pd.DataFrame(flat_stats, index=range(n_windows)).describe().T\n",
        "\n",
        "datasets = {\n",
        "    \"components\": components_df,\n",
        "    \"prices\": prices_df\n",
        "}\n",
        "\n",
        "summary_stats = {name: df.describe().T for name, df in datasets.items()}\n",
        "missing_values = {name: df.isna().sum() for name, df in datasets.items()}\n",
        "\n",
        "upper_triu = np.triu_indices(corr_array.shape[1], k=1)\n",
        "all_corrs = corr_array[:, upper_triu[0], upper_triu[1]].flatten()\n",
        "corr_summary_flat = pd.Series(all_corrs).describe()\n",
        "\n",
        "dist_summary = summarize_3d_array(distance_array, \"distance\")\n",
        "\n",
        "spearman_tests = {}\n",
        "kendall_tests = {}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if df.shape[1] > 1:\n",
        "        spearman_tests[name] = df.corr(method='spearman')\n",
        "        kendall_tests[name] = df.corr(method='kendall')\n",
        "\n",
        "pca_results = {}\n",
        "for name, df in datasets.items():\n",
        "    if df.shape[1] > 1:\n",
        "        df_clean = df.dropna()\n",
        "        if not df_clean.empty:\n",
        "            pca = PCA().fit(df_clean)\n",
        "            pca_results[name] = {\n",
        "                \"explained_variance\": pca.explained_variance_ratio_,\n",
        "                \"components\": pca.components_\n",
        "            }\n",
        "\n",
        "print(\"Summary stats for 2D datasets:\")\n",
        "for name, stats in summary_stats.items():\n",
        "    print(f\"\\n{name}:\\n\", stats)\n",
        "\n",
        "print(\"\\nFlattened correlation summary:\")\n",
        "print(corr_summary_flat)\n",
        "\n",
        "print(\"\\nDistance 3D summary:\")\n",
        "print(dist_summary)"
      ],
      "metadata": {
        "id": "Yf3c0jMKKjwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "components_per_day = components_df.notna().sum(axis=1)\n",
        "components_per_day.index = pd.to_datetime(components_df.index)\n",
        "components_monthly = components_per_day.resample('M').mean()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "components_monthly.plot()\n",
        "plt.title(\"Average Number of S&P 500 Components per Month\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Number of Components\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "n_windows = corr_array.shape[0]\n",
        "high_corr_counts = [np.sum(np.triu(corr_array[i], k=1) > 0.5) for i in range(n_windows)]\n",
        "window_dates = returns.index[window-1:]\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(window_dates, high_corr_counts)\n",
        "plt.title(\"Number of Stock Pairs with Correlation > 0.5 Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Pairs\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(corr_array[0][np.triu_indices(corr_array.shape[1], k=1)], bins=50, kde=True)\n",
        "plt.title(\"Distribution of Correlations (First Rolling Window)\")\n",
        "plt.xlabel(\"Correlation\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(distance_array[0][np.triu_indices(distance_array.shape[1], k=1)], bins=50, kde=True)\n",
        "plt.title(\"Distribution of Distances (First Rolling Window)\")\n",
        "plt.xlabel(\"Distance\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lhMm5msGZtT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rolling Windows"
      ],
      "metadata": {
        "id": "FQJj3bD5c7cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 252\n",
        "start_date = '1996-01-01'\n",
        "end_date = '2002-12-31'\n",
        "\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='W-FRI')\n",
        "n_windows = len(dates)\n",
        "\n",
        "for date in dates:\n",
        "    window_idx = np.searchsorted(window_dates, date, side='right') - 1\n",
        "    if window_idx < 0 or window_idx >= len(corr_array):\n",
        "        continue\n",
        "\n",
        "    corr_mat = corr_array[window_idx]\n",
        "    dist_mat = distance_array[window_idx]\n",
        "\n",
        "    upper_triu = np.triu_indices(corr_mat.shape[0], k=1)\n",
        "\n",
        "    avg_corr = np.nanmean(corr_mat[upper_triu])\n",
        "    high_corr_count = np.sum(corr_mat[upper_triu] > 0.5)\n",
        "\n",
        "    print(f\"Week ending {date.date()}: Avg Corr={avg_corr:.3f}, High Corr Pairs={high_corr_count}\")\n",
        "\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.histplot(corr_mat[upper_triu], bins=30, kde=True)\n",
        "    plt.title(f\"Correlation Distribution: Week ending {date.date()}\")\n",
        "    plt.xlabel(\"Correlation\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.histplot(dist_mat[upper_triu], bins=30, kde=True)\n",
        "    plt.title(f\"Distance Distribution: Week ending {date.date()}\")\n",
        "    plt.xlabel(\"Distance\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SqgaC8iKc7yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "for i, date in enumerate(window_dates):\n",
        "    if i % 4 != 0:\n",
        "        continue\n",
        "    corr_mat = corr_array[i]\n",
        "    corr_vals = corr_mat[np.triu_indices(corr_mat.shape[0], k=1)]\n",
        "    sns.kdeplot(corr_vals, label=str(date.date()), alpha=0.5)\n",
        "\n",
        "plt.title(\"Correlation Distributions Across Weeks\")\n",
        "plt.xlabel(\"Correlation\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend(title=\"Week Ending\", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=1, fontsize=8)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for i, date in enumerate(window_dates):\n",
        "    if i % 4 != 0:\n",
        "        continue\n",
        "    dist_mat = distance_array[i]\n",
        "    dist_vals = dist_mat[np.triu_indices(dist_mat.shape[0], k=1)]\n",
        "    sns.kdeplot(dist_vals, label=str(date.date()), alpha=0.5)\n",
        "\n",
        "plt.title(\"Distance Distributions Across Weeks\")\n",
        "plt.xlabel(\"Distance\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend(title=\"Week Ending\", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=1, fontsize=8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C0qE1wemd4zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 252\n",
        "start_date = '1996-01-01'\n",
        "end_date = '2002-12-31'\n",
        "\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='W-FRI')\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "for date in dates:\n",
        "    window_idx = np.searchsorted(window_dates, date, side='right') - 1\n",
        "    if window_idx < 0 or window_idx >= len(corr_array):\n",
        "        continue\n",
        "\n",
        "    corr_mat = corr_array[window_idx]\n",
        "    upper_triu = np.triu_indices(corr_mat.shape[0], k=1)\n",
        "    sns.kdeplot(corr_mat[upper_triu], alpha=0.3, linewidth=1)\n",
        "\n",
        "plt.title(\"Correlation Distributions Across Weekly Rolling Windows\")\n",
        "plt.xlabel(\"Correlation\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "for date in dates:\n",
        "    window_idx = np.searchsorted(window_dates, date, side='right') - 1\n",
        "    if window_idx < 0 or window_idx >= len(distance_array):\n",
        "        continue\n",
        "\n",
        "    dist_mat = distance_array[window_idx]\n",
        "    upper_triu = np.triu_indices(dist_mat.shape[0], k=1)\n",
        "    sns.kdeplot(dist_mat[upper_triu], alpha=0.3, linewidth=1)\n",
        "\n",
        "plt.title(\"Distance Distributions Across Weekly Rolling Windows\")\n",
        "plt.xlabel(\"Distance\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zjwBz2t6Kyi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vietoris-Rips Complexes, Betti Numbers, \\& Persistence Diagrams**"
      ],
      "metadata": {
        "id": "jWOgvYWiWrhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_betti_numbers(dist_mat, maxdim=2):\n",
        "    diagrams = ripser(dist_mat, distance_matrix=True, maxdim=maxdim)['dgms']\n",
        "    betti_counts = [len(dg[~np.isinf(dg[:,1])]) for dg in diagrams]  # counts per dimension\n",
        "    return betti_counts, diagrams\n",
        "\n",
        "window_size = 252\n",
        "start_date = '1996-01-01'\n",
        "end_date = '2002-12-31'\n",
        "\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='W-FRI')\n",
        "\n",
        "betti_results = []\n",
        "\n",
        "for date in dates:\n",
        "    window_idx = np.searchsorted(window_dates, date, side='right') - 1\n",
        "    if window_idx < 0 or window_idx >= len(distance_array):\n",
        "        continue\n",
        "\n",
        "    dist_mat = distance_array[window_idx]\n",
        "    betti_counts, diagrams = compute_betti_numbers(dist_mat, maxdim=2)\n",
        "    betti_results.append({\n",
        "        'date': date,\n",
        "        'betti_0': betti_counts[0],\n",
        "        'betti_1': betti_counts[1],\n",
        "        'betti_2': betti_counts[2]\n",
        "    })\n",
        "\n",
        "betti_df = pd.DataFrame(betti_results).set_index('date')\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(betti_df.index, betti_df['betti_0'], label='Betti-0')\n",
        "plt.plot(betti_df.index, betti_df['betti_1'], label='Betti-1')\n",
        "plt.plot(betti_df.index, betti_df['betti_2'], label='Betti-2')\n",
        "plt.title(\"Betti Numbers Over Time (Vietoris-Rips Complexes)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kKvd3aeSWvZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persistence Landscapes**"
      ],
      "metadata": {
        "id": "Y0r3OnzYWy1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_persistence_landscape(dist_mat, maxdim=1):\n",
        "    diagrams = ripser(dist_mat, distance_matrix=True, maxdim=maxdim)['dgms']\n",
        "    landscapes = {}\n",
        "    for dim in range(maxdim + 1):\n",
        "        pl = PersLandscapeExact(dgms=[diagrams[dim]])\n",
        "        landscapes[f'landscape_dim{dim}'] = pl\n",
        "    return landscapes, diagrams\n",
        "\n",
        "window_size = 252\n",
        "dates = pd.date_range(start='1996-01-01', end='2002-12-31', freq='W-FRI')\n",
        "\n",
        "landscape_results = []\n",
        "\n",
        "for date in dates:\n",
        "    window_idx = np.searchsorted(window_dates, date, side='right') - 1\n",
        "    if window_idx < 0 or window_idx >= len(distance_array):\n",
        "        continue\n",
        "\n",
        "    dist_mat = distance_array[window_idx]\n",
        "    landscapes, diagrams = compute_persistence_landscape(dist_mat, maxdim=1)\n",
        "    landscape_results.append({\n",
        "        'date': date,\n",
        "        'landscapes': landscapes,\n",
        "        'diagrams': diagrams\n",
        "    })\n",
        "\n",
        "from persim.landscapes import PersLandscapeExact, plot_landscape_simple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "first_week = landscape_results[0]\n",
        "dgms = first_week['diagrams']\n",
        "\n",
        "if len(dgms) > 1 and dgms[1].size > 0:\n",
        "    landscape_dim1 = PersLandscapeExact(dgms=dgms, hom_deg=1)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plot_landscape_simple(landscape_dim1)\n",
        "    plt.title(f'Persistence Landscape (Dimension 1) - Week ending {first_week[\"date\"].date()}')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No dimension-1 features for this week.\")"
      ],
      "metadata": {
        "id": "0t8wlNy2WxLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "for week_data in landscape_results:\n",
        "    dgms = week_data['diagrams']\n",
        "    if len(dgms) > 1 and dgms[1].size > 0:\n",
        "        landscape = PersLandscapeExact(dgms=dgms, hom_deg=1)\n",
        "        plot_landscape_simple(landscape, alpha=0.3)\n",
        "\n",
        "plt.title(\"Overlay of Dimension-1 Persistence Landscapes Across Weeks\")\n",
        "plt.xlabel(\"Filtration Value\")\n",
        "plt.ylabel(\"Landscape Value\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p7ygRgdEs9lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persistence Images**"
      ],
      "metadata": {
        "id": "Q0XhwvxjW0LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi = PersistenceImager(pixel_size=0.1)\n",
        "\n",
        "persistence_images = []\n",
        "\n",
        "for week_data in landscape_results:\n",
        "    dgms = week_data['diagrams']\n",
        "    if len(dgms) > 1 and dgms[1].size > 0:\n",
        "        diagram = dgms[1]\n",
        "        pi.fit(diagram)\n",
        "        img = pi.transform(diagram)\n",
        "        persistence_images.append({\n",
        "            'date': week_data['date'],\n",
        "            'image': img\n",
        "        })\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(persistence_images[0]['image'], origin='lower', cmap='viridis', aspect='auto')\n",
        "plt.colorbar(label='Intensity')\n",
        "plt.title(f'Persistence Image (Dimension 1) - Week ending {persistence_images[0][\"date\"].date()}')\n",
        "plt.xlabel('X-axis (birth/persistence)')\n",
        "plt.ylabel('Y-axis (persistence)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Csu7Nn1pW07b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim1_diagrams = [week['diagrams'][1] for week in landscape_results if len(week['diagrams']) > 1 and week['diagrams'][1].size > 0]\n",
        "\n",
        "pi = PersistenceImager(pixel_size=0.1)\n",
        "pi.fit(np.vstack(dim1_diagrams))\n",
        "\n",
        "all_images = []\n",
        "for diagram in dim1_diagrams:\n",
        "    img = pi.transform(diagram)\n",
        "    all_images.append(img)\n",
        "\n",
        "all_images_array = np.stack(all_images)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for img in all_images_array:\n",
        "    plt.imshow(img, origin='lower', cmap='viridis', alpha=0.1, aspect='auto')\n",
        "plt.colorbar(label='Intensity')\n",
        "plt.title('Overlay of Persistence Images (Dimension 1) Across All Weeks')\n",
        "plt.xlabel('X-axis (birth/persistence)')\n",
        "plt.ylabel('Y-axis (persistence)')\n",
        "plt.show()\n",
        "\n",
        "avg_image = np.mean(all_images_array, axis=0)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(avg_image, origin='lower', cmap='viridis', aspect='auto')\n",
        "plt.colorbar(label='Average Intensity')\n",
        "plt.title('Average Persistence Image (Dimension 1) Across All Weeks')\n",
        "plt.xlabel('X-axis (birth/persistence)')\n",
        "plt.ylabel('Y-axis (persistence)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o-mrkKBYOqYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Clustering & Results**"
      ],
      "metadata": {
        "id": "naVNbSBHW1ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_list = []\n",
        "\n",
        "for i, week in enumerate(landscape_results):\n",
        "    features = {}\n",
        "\n",
        "    features['betti_0'] = week['betti_0'] if 'betti_0' in week else np.nan\n",
        "    features['betti_1'] = week['betti_1'] if 'betti_1' in week else np.nan\n",
        "    features['betti_2'] = week['betti_2'] if 'betti_2' in week else np.nan\n",
        "\n",
        "    pl = week['landscapes'].get('landscape_dim1', None)\n",
        "    if pl:\n",
        "        features['pl_supnorm'] = pl.sup_norm()\n",
        "        features['pl_l2norm'] = pl.p_norm(p=2)\n",
        "    else:\n",
        "        features['pl_supnorm'] = np.nan\n",
        "        features['pl_l2norm'] = np.nan\n",
        "\n",
        "    if 'persistence_image_dim1' in week:\n",
        "        img = week['persistence_image_dim1']\n",
        "        features.update({f'pi_{j}': val for j, val in enumerate(img.flatten())})\n",
        "\n",
        "    feature_list.append(features)\n",
        "\n",
        "feature_df = pd.DataFrame(feature_list, index=[week['date'] for week in landscape_results])\n",
        "feature_df = feature_df.fillna(0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(feature_df)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=clusters, palette='Set2')\n",
        "plt.title('Clustering of Weeks Based on Topological Features')\n",
        "plt.xlabel('PCA 1')\n",
        "plt.ylabel('PCA 2')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "feature_df['cluster'] = clusters\n",
        "\n",
        "for c in sorted(feature_df['cluster'].unique()):\n",
        "    print(f\"\\nCluster {c} Summary:\")\n",
        "    cluster_data = feature_df[feature_df['cluster'] == c].drop(columns='cluster')\n",
        "    summary_stats = cluster_data.agg(['mean', 'max', 'min', 'std']).transpose()\n",
        "    print(summary_stats)"
      ],
      "metadata": {
        "id": "wZuuaNJzW4HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = betti_df[['betti_0', 'betti_1', 'betti_2']].copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features_df)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "features_df['cluster'] = cluster_labels\n",
        "features_df['date'] = features_df.index\n",
        "\n",
        "synthetic_index = pd.Series(predicted, index=spx_log_returns.index)\n",
        "cumulative_index = (1 + synthetic_index).cumprod()\n",
        "\n",
        "cumulative_index_weekly = cumulative_index.resample('W-FRI').last()\n",
        "\n",
        "cumulative_index_weekly = cumulative_index_weekly.reindex(features_df['date'])\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.plot(cumulative_index_weekly.index, cumulative_index_weekly.values, color='blue', label='Synthetic Index', linewidth=2)\n",
        "\n",
        "cluster_colors = {0: 'green', 1: 'orange', 2: 'red'}\n",
        "for cluster in np.unique(cluster_labels):\n",
        "    cluster_dates = features_df[features_df['cluster'] == cluster]['date']\n",
        "    plt.scatter(cluster_dates,\n",
        "                cumulative_index_weekly.loc[cluster_dates],\n",
        "                color=cluster_colors[cluster],\n",
        "                alpha=0.5,\n",
        "                label=f'Cluster {cluster}')\n",
        "\n",
        "plt.title(\"Market Clusters Over Time with Synthetic Index\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Cumulative Index Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wRrFGjWWQ6aA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
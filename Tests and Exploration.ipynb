{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmiKPMT1pUyg"
      },
      "source": [
        "**Dependencies & Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-yOPHBtpSUb"
      },
      "outputs": [],
      "source": [
        "!pip install pandas_market_calendars ripser persim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knYuTJUopYtC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import MDS\n",
        "import statsmodels.api as sm\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import networkx as nx\n",
        "from matplotlib.colors import Normalize\n",
        "from scipy.spatial.distance import squareform\n",
        "from tqdm import tqdm\n",
        "import yfinance as yf\n",
        "import pandas_datareader.data as web\n",
        "import pandas_market_calendars as mcal\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.cm as cm\n",
        "from ripser import ripser\n",
        "import networkx as nx\n",
        "from numpy.polynomial.polynomial import Polynomial\n",
        "from persim import plot_diagrams\n",
        "from persim import PersLandscapeExact\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Processing**"
      ],
      "metadata": {
        "id": "Y2vn9ElAxXvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny1-wt72pupJ"
      },
      "outputs": [],
      "source": [
        "components_df = pd.read_csv('/content/components.csv')\n",
        "\n",
        "components_df['date'] = pd.to_datetime(components_df['date'])\n",
        "components_df = components_df.set_index(\"date\").sort_index()\n",
        "\n",
        "nyse = mcal.get_calendar('NYSE')\n",
        "schedule = nyse.schedule(start_date=components_df.index.min(), end_date=components_df.index.max())\n",
        "trading_days = pd.DatetimeIndex(schedule.index.date)\n",
        "\n",
        "components_df = components_df.reindex(trading_days)\n",
        "\n",
        "components_df['tickers'] = components_df['tickers'].str.split(\",\")\n",
        "components_df['tickers'] = components_df['tickers'].ffill()\n",
        "\n",
        "components_df = components_df.reset_index().rename(columns={'index': 'date'})\n",
        "\n",
        "components_df = components_df.set_index('date')\n",
        "\n",
        "components_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLwFq5iTpv9C"
      },
      "outputs": [],
      "source": [
        "prices_raw = pd.read_csv('/content/prices.csv')\n",
        "\n",
        "prices_raw['date'] = pd.to_datetime(prices_raw['date'])\n",
        "\n",
        "prices_df = prices_raw.pivot_table(index='date', columns='TICKER', values='PRC', aggfunc='first')\n",
        "\n",
        "prices_df = prices_df.sort_index(axis=1)\n",
        "\n",
        "prices_df = prices_df.abs()\n",
        "\n",
        "mask = pd.DataFrame(False, index=prices_df.index, columns=prices_df.columns)\n",
        "\n",
        "for date, row in components_df.iterrows():\n",
        "    tickers = row['tickers']\n",
        "\n",
        "    valid = list(set(tickers) & set(prices_df.columns))\n",
        "    mask.loc[date, valid] = True\n",
        "\n",
        "prices_df = prices_df.where(mask)\n",
        "\n",
        "spx = yf.download('^GSPC', start='1996-01-01', end='2002-12-31')['Close']\n",
        "prices_df.index = pd.to_datetime(prices_df.index)\n",
        "spx.index = pd.to_datetime(spx.index)\n",
        "prices_df = prices_df.join(spx, how='left')\n",
        "\n",
        "prices_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Sejrrtcsvq-8"
      },
      "outputs": [],
      "source": [
        "spx_log_returns = np.log(spx / spx.shift(1)).dropna()\n",
        "component_log_returns = np.log(prices_df / prices_df.shift(1))\n",
        "\n",
        "component_log_returns = component_log_returns.loc[spx_log_returns.index]\n",
        "component_log_returns = component_log_returns.fillna(0)\n",
        "\n",
        "X = component_log_returns\n",
        "y = spx_log_returns\n",
        "\n",
        "model_no_alpha = sm.OLS(y, X).fit()\n",
        "print(model_no_alpha.summary())\n",
        "\n",
        "predicted = model_no_alpha.predict(X).to_numpy().flatten()\n",
        "y_flat = y.to_numpy().flatten()\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Actual': y_flat,\n",
        "    'Synthetic': predicted\n",
        "})\n",
        "\n",
        "ax = comparison.cumsum().plot(title='Cumulative S&P 500 Log Return: Actual vs. Synthetic')\n",
        "ax.set_xlabel(\"Day\")\n",
        "ax.set_ylabel(\"Cumulative Log Return\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXkxRtCUAfMl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_flat, predicted, alpha=0.5)\n",
        "plt.xlabel(\"Actual SPX Log Return\")\n",
        "plt.ylabel(\"Synthetic SPX Log Return\")\n",
        "plt.title(\"Actual vs Synthetic SPX Log Returns\")\n",
        "plt.plot([y_flat.min(), y_flat.max()], [y_flat.min(), y_flat.max()], color='red', lw=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntSK9tGkCsr9"
      },
      "source": [
        "**Correlation & Distance Matrices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CSArrToCd8V"
      },
      "outputs": [],
      "source": [
        "window = 126\n",
        "rolling_corrs_dict = {}\n",
        "\n",
        "for t in range(window-1, len(prices_df)):\n",
        "    window_data = prices_df.iloc[t-window+1:t+1]\n",
        "    rolling_corrs_dict[prices_df.index[t]] = window_data.corr()\n",
        "\n",
        "example_date = prices_df.index[window-1]\n",
        "print(rolling_corrs_dict[example_date])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI79sIR-LfZv"
      },
      "outputs": [],
      "source": [
        "corr_matrix = prices_df.corr()\n",
        "distance_matrix = np.sqrt(2 * (1 - corr_matrix))\n",
        "print(distance_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Traditional Data Exploration**"
      ],
      "metadata": {
        "id": "IgbotuYuxivP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8RXOFCKML9A"
      },
      "outputs": [],
      "source": [
        "daily_stats = prices_df.describe()\n",
        "print(daily_stats)\n",
        "\n",
        "log_returns = np.log(prices_df / prices_df.shift(1))\n",
        "daily_volatility = log_returns.std(axis=1)\n",
        "\n",
        "daily_volatility.plot(title=\"Daily Volatility of Components\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Volatility\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQwbJVASMN5M"
      },
      "outputs": [],
      "source": [
        "components_count = components_df['tickers'].apply(len)\n",
        "components_count.plot(title=\"Number of Components in S&P 500 Over Time\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7jEdYO7MQe0"
      },
      "outputs": [],
      "source": [
        "window = 22\n",
        "\n",
        "rolling_mean = prices_df.rolling(window).mean()\n",
        "rolling_vol = prices_df.rolling(window).std()\n",
        "\n",
        "rolling_vol.mean(axis=1).plot(title=\"Average Rolling Volatility of Components\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA9ZWYaLMWkI"
      },
      "outputs": [],
      "source": [
        "distance_matrix = np.sqrt(2 * (1 - corr_matrix))\n",
        "\n",
        "avg_distance = distance_matrix.mean().mean()\n",
        "print(f\"Average distance: {avg_distance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94BB2MkOMaX4"
      },
      "outputs": [],
      "source": [
        "log_returns_filled = np.log(prices_df / prices_df.shift(1)).fillna(0)\n",
        "pca = PCA(n_components=919)\n",
        "pca.fit(log_returns_filled)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained variance by all components:\", explained_variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktpP-D4sMgMk"
      },
      "outputs": [],
      "source": [
        "window = 60\n",
        "rolling_pc1_var = []\n",
        "\n",
        "for t in range(window, len(log_returns_filled)):\n",
        "    window_data = log_returns_filled.iloc[t-window:t]\n",
        "    pca = PCA(n_components=1)\n",
        "    pca.fit(window_data)\n",
        "    rolling_pc1_var.append(pca.explained_variance_ratio_[0])\n",
        "\n",
        "pd.Series(rolling_pc1_var, index=log_returns_filled.index[window:]).plot(title=\"Rolling Variance Explained by PC1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6mg7Y4Davil"
      },
      "outputs": [],
      "source": [
        "log_returns_filled = np.log(prices_df / prices_df.shift(1)).fillna(0)\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(log_returns_filled.T)\n",
        "\n",
        "inertia = []\n",
        "k_values = range(2, 15)\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(pca_result)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(k_values, inertia, marker='o')\n",
        "plt.xlabel(\"Number of clusters k\")\n",
        "plt.ylabel(\"Inertia (Sum of Squared Distances)\")\n",
        "plt.title(\"Elbow Method for Optimal k\")\n",
        "plt.show()\n",
        "\n",
        "sil_scores = []\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(pca_result)\n",
        "    score = silhouette_score(pca_result, labels)\n",
        "    sil_scores.append(score)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(k_values, sil_scores, marker='o')\n",
        "plt.xlabel(\"Number of clusters k\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Scores for Different k\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIr7rFa4aYQ6"
      },
      "outputs": [],
      "source": [
        "log_returns_filled = np.log(prices_df / prices_df.shift(1)).fillna(0)\n",
        "\n",
        "pca = PCA(n_components=919)\n",
        "pca_result = pca.fit_transform(log_returns_filled.T)\n",
        "\n",
        "k = 2\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "clusters = kmeans.fit_predict(pca_result)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "for cluster_id in range(k):\n",
        "    cluster_points = pca_result[clusters == cluster_id]\n",
        "    plt.scatter(cluster_points[:,0], cluster_points[:,1], label=f\"Cluster {cluster_id+1}\", alpha=0.7)\n",
        "\n",
        "for i, ticker in enumerate(log_returns_filled.columns):\n",
        "    plt.text(pca_result[i,0], pca_result[i,1], ticker, fontsize=8)\n",
        "\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA Clustering of S&P 500 Components\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kOqhK75bVTI"
      },
      "outputs": [],
      "source": [
        "log_returns_filled = np.log(prices_df / prices_df.shift(1)).fillna(0)\n",
        "pca = PCA(n_components=919)\n",
        "pca_result = pca.fit_transform(log_returns_filled.T)\n",
        "\n",
        "k = 2\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "clusters = kmeans.fit_predict(pca_result)\n",
        "\n",
        "cluster_df = pd.DataFrame({\n",
        "    'Ticker': log_returns_filled.columns,\n",
        "    'Cluster': clusters\n",
        "})\n",
        "\n",
        "stats_list = []\n",
        "for cluster_id in range(k):\n",
        "    tickers_in_cluster = cluster_df[cluster_df['Cluster'] == cluster_id]['Ticker']\n",
        "    cluster_returns = log_returns_filled[tickers_in_cluster]\n",
        "\n",
        "    mean_return = cluster_returns.mean(axis=1).mean()\n",
        "    volatility = cluster_returns.std(axis=1).mean()\n",
        "    avg_correlation = cluster_returns.corr().mean().mean()\n",
        "\n",
        "    stats_list.append({\n",
        "        'Cluster': cluster_id,\n",
        "        'Num_Tickers': len(tickers_in_cluster),\n",
        "        'Mean_Return': mean_return,\n",
        "        'Volatility': volatility,\n",
        "        'Avg_Correlation': avg_correlation\n",
        "    })\n",
        "\n",
        "cluster_stats = pd.DataFrame(stats_list)\n",
        "print(cluster_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-JZ9SLWMmYX"
      },
      "outputs": [],
      "source": [
        "selected_date = '2000-03-10'\n",
        "if selected_date in prices_df.index:\n",
        "    corr_matrix_date = prices_df.loc[:selected_date].tail(window).corr()\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(corr_matrix_date, cmap='coolwarm', center=0)\n",
        "    plt.title(f\"Correlation Matrix around {selected_date}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3annqNdMi7B"
      },
      "outputs": [],
      "source": [
        "window = 126\n",
        "\n",
        "avg_corr = log_returns.rolling(window).corr()\n",
        "avg_corr = avg_corr.groupby(level=0).mean()\n",
        "avg_vol = log_returns.rolling(window).std().mean(axis=1)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(avg_vol.index, avg_vol, label=\"Average Volatility\")\n",
        "plt.plot(avg_corr.index, avg_corr, label=\"Average Correlation\")\n",
        "plt.title(\"Volatility vs Correlation Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huRUzqBuEAK0"
      },
      "source": [
        "**VC Complexes & Betti Numbers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzcR701md_xw"
      },
      "outputs": [],
      "source": [
        "window_size = 5\n",
        "maxdim = 2\n",
        "betti_results = []\n",
        "\n",
        "for t in range(window_size - 1, len(prices_df)):\n",
        "    window_data = prices_df.iloc[t-window_size+1:t+1].fillna(0)\n",
        "    variances = window_data.var()\n",
        "    sorted_stocks = variances.sort_values(ascending=False)\n",
        "\n",
        "    cumsum_var = sorted_stocks.cumsum() / sorted_stocks.sum()\n",
        "    alpha = 0.85\n",
        "    top_stocks = cumsum_var[cumsum_var <= alpha].index\n",
        "\n",
        "    window_data = window_data[top_stocks]\n",
        "\n",
        "    if window_data.shape[1] < 2:\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing window ending on {prices_df.index[t]} with {window_data.shape[1]} tickers\")\n",
        "\n",
        "    corr_matrix = window_data.corr()\n",
        "    dist_matrix = np.sqrt(np.clip(2 * (1 - corr_matrix), 0, 2))\n",
        "\n",
        "    try:\n",
        "        diagrams = ripser(dist_matrix.values, distance_matrix=True, maxdim=maxdim)['dgms']\n",
        "        betti_0 = len(diagrams[0][~np.isinf(diagrams[0][:,1])])\n",
        "        betti_1 = len(diagrams[1][~np.isinf(diagrams[1][:,1])])\n",
        "    except Exception as e:\n",
        "        print(f\"Ripser failed at {prices_df.index[t]}: {e}\")\n",
        "        betti_0 = betti_1 = np.nan\n",
        "\n",
        "    betti_results.append({\n",
        "        'date': prices_df.index[t],\n",
        "        'betti_0': betti_0,\n",
        "        'betti_1': betti_1,\n",
        "        'num_top_stocks': len(top_stocks)\n",
        "    })\n",
        "\n",
        "betti_df = pd.DataFrame(betti_results).set_index('date')\n",
        "print(betti_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "woYw8NpgsKEk"
      },
      "outputs": [],
      "source": [
        "window_data = prices_df.fillna(0)\n",
        "\n",
        "corr_matrix = window_data.corr()\n",
        "dist_matrix = np.sqrt(np.clip(2 * (1 - corr_matrix), 0, 2))\n",
        "\n",
        "D = dist_matrix.values\n",
        "nodes = list(dist_matrix.index)\n",
        "\n",
        "thresholds = np.linspace(0, np.nanmax(D), 10)\n",
        "\n",
        "for eps in thresholds:\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(nodes)\n",
        "\n",
        "    for i in range(len(nodes)):\n",
        "        for j in range(i+1, len(nodes)):\n",
        "            if D[i, j] <= eps:\n",
        "                G.add_edge(nodes[i], nodes[j])\n",
        "\n",
        "    plt.figure(figsize=(7,7))\n",
        "    nx.draw(\n",
        "        G,\n",
        "        with_labels=False,\n",
        "        node_size=80,\n",
        "        node_color='skyblue',\n",
        "        edge_color='gray',\n",
        "        linewidths=1.0,\n",
        "        edgecolors='black'\n",
        "    )\n",
        "    plt.title(f\"Vietoris–Rips 1-skeleton (ε = {eps:.2f})\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcc5qGIpEEDh"
      },
      "outputs": [],
      "source": [
        "dates_num = np.arange(len(betti_df))\n",
        "betti0 = betti_df['betti_0'].values\n",
        "betti1 = betti_df['betti_1'].values\n",
        "\n",
        "p0 = Polynomial.fit(dates_num, betti0, 10)\n",
        "p1 = Polynomial.fit(dates_num, betti1, 10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(betti_df.index, betti0, alpha=0.5, label='Betti-0')\n",
        "plt.plot(betti_df.index, betti1, alpha=0.5, label='Betti-1')\n",
        "plt.plot(betti_df.index, p0(dates_num), label='Betti-0 Trend (Poly deg 3)', color='blue', lw=2)\n",
        "plt.plot(betti_df.index, p1(dates_num), label='Betti-1 Trend (Poly deg 3)', color='orange', lw=2)\n",
        "plt.title(\"Betti Numbers Over Time with Polynomial Trend\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSIMkS2wK0IS"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(betti_df.index, betti_df['betti_0'] / betti_df['num_top_stocks'], label='Normalized Betti-0')\n",
        "plt.plot(betti_df.index, betti_df['betti_1'] / betti_df['num_top_stocks'], label='Normalized Betti-1')\n",
        "plt.title(\"Normalized Betti Numbers Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Normalized Count\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persistence Diagrams & Landscape**"
      ],
      "metadata": {
        "id": "UvozhVqOyX1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZPq9j4PHOPr"
      },
      "outputs": [],
      "source": [
        "window_data = prices_df.fillna(0)\n",
        "\n",
        "variances = window_data.var()\n",
        "sorted_stocks = variances.sort_values(ascending=False)\n",
        "cumsum_var = sorted_stocks.cumsum() / sorted_stocks.sum()\n",
        "\n",
        "alpha = 0.85\n",
        "top_stocks = cumsum_var[cumsum_var <= alpha].index\n",
        "\n",
        "window_data = window_data[top_stocks]\n",
        "\n",
        "corr_matrix = window_data.corr()\n",
        "dist_matrix = np.sqrt(np.clip(2 * (1 - corr_matrix), 0, 2))\n",
        "\n",
        "diagrams = ripser(dist_matrix.values, distance_matrix=True, maxdim=2)['dgms']\n",
        "\n",
        "plot_diagrams(diagrams, show=False)\n",
        "plt.title(f\"Persistence Diagram of Stocks Explaining {int(alpha*100)}% Variance ({len(top_stocks)} stocks)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHxL4a5SHOhZ"
      },
      "outputs": [],
      "source": [
        "ple = PersLandscapeExact(dgms=diagrams, hom_deg=1)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "for layer_idx, layer in enumerate(ple.critical_pairs):\n",
        "    layer = np.array(layer)\n",
        "    x = layer[:,0]\n",
        "    y = layer[:,1]\n",
        "    plt.plot(x, y, label=f\"Layer {layer_idx+1}\")\n",
        "\n",
        "plt.title(\"Exact Persistence Landscape (H1)\")\n",
        "plt.xlabel(\"Filtration (epsilon)\")\n",
        "plt.ylabel(\"Landscape value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapper Algorithm & Hierarchical Clustering**"
      ],
      "metadata": {
        "id": "HnLkeV7i1uwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = betti_df[['betti_0', 'betti_1']].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "mapper = km.KeplerMapper()\n",
        "\n",
        "lens = PCA(n_components=2).fit_transform(data_scaled)\n",
        "\n",
        "graph = mapper.map(\n",
        "    lens,\n",
        "    data_scaled,\n",
        "    clusterer=km.cluster.DBSCAN(eps=0.5, min_samples=3),\n",
        "    cover=km.Cover(n_cubes=10, perc_overlap=0.3)\n",
        ")\n",
        "\n",
        "mapper.visualize(\n",
        "    graph,\n",
        "    path_html=\"mapper_output.html\",\n",
        "    title=\"Mapper Algorithm on Dataset\"\n",
        ")\n",
        "\n",
        "print(\"Mapper graph generated! Open 'mapper_output.html' to view the visualization.\")"
      ],
      "metadata": {
        "id": "QJVYkpiq1tc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = betti_df[['betti_0', 'betti_1']].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram = sch.dendrogram(sch.linkage(data_scaled, method='ward'))\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Euclidean distance\")\n",
        "plt.show()\n",
        "\n",
        "n_clusters = 3\n",
        "hc = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n",
        "labels = hc.fit_predict(data_scaled)\n",
        "\n",
        "print(\"Cluster labels:\", labels)"
      ],
      "metadata": {
        "id": "febknIzW1twh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}